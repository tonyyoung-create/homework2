{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c535fa",
   "metadata": {},
   "source": [
    "# CRISP-DM: Spam Email Detection\n",
    "\n",
    "This notebook follows the CRISP-DM process for the Spam Email Detection project. It demonstrates data understanding, preprocessing, modeling, evaluation, and saving artifacts for deployment. The notebook is runnable with the lightweight `data/sample_emails.csv` present in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b4363",
   "metadata": {},
   "source": [
    "## 1. 環境與相依套件安裝\n",
    "Install dependencies (only if needed) and show Python and package versions. In CI you would normally install from `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment checks\n",
    "import sys\n",
    "import pkg_resources\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "packages = ['pandas','numpy','sklearn','joblib','streamlit']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        ver = pkg_resources.get_distribution(pkg).version\n",
    "        print(f'{pkg}: {ver}')\n",
    "    except Exception:\n",
    "        print(f'{pkg}: not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836dcd2",
   "metadata": {},
   "source": [
    "## 2. 載入與檢查資料檔案\n",
    "Load the dataset from `data/data.csv` (if present) or fall back to `data/sample_emails.csv`. Show basic file checks with `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "DATA_PATH = Path('data') / 'data.csv'\n",
    "SAMPLE_PATH = Path('data') / 'sample_emails.csv'\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Loaded data.csv')\n",
    "else:\n",
    "    df = pd.read_csv(SAMPLE_PATH)\n",
    "    print('Loaded sample_emails.csv')\n",
    "print('\\nData shape:', df.shape)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9433b",
   "metadata": {},
   "source": [
    "## 3. 初步資料檢視與摘要統計\n",
    "Use `head`, `info`, `describe`, compute missing-value ratios and display value counts for the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc02141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nInfo:')\n",
    "df.info()\n",
    "print('\\nDescribe:')\n",
    "print(df.describe(include='all'))\n",
    "print('\\nMissing value ratios:')\n",
    "print(df.isnull().mean())\n",
    "if 'label' in df.columns:\n",
    "    print('\\nLabel distribution:')\n",
    "    print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81be9bc",
   "metadata": {},
   "source": [
    "## 4. 資料清理：遺失值、重複與資料型態\n",
    "Apply simple cleaning: drop exact duplicates and fill missing text with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73312efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f'Removed {before-after} duplicate rows')\n",
    "if 'text' in df.columns:\n",
    "    df['text'] = df['text'].fillna('')\n",
    "if 'label' in df.columns:\n",
    "    df['label'] = df['label'].astype('category')\n",
    "    print('\\nLabel categories:', df['label'].cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421fac6d",
   "metadata": {},
   "source": [
    "## 5. 資料視覺化探索（EDA）\n",
    "Plot basic distributions: label counts and word count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df['word_count'] = df['text'].apply(lambda s: len(str(s).split()))\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.title('Label distribution')\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['word_count'], bins=20)\n",
    "plt.title('Word count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37fe7d",
   "metadata": {},
   "source": [
    "## 6. 特徵工程與處理（編碼、標準化、衍生特徵）\n",
    "We will use the preprocessing utilities from `src.preprocessing`. For text we'll use TF-IDF vectorization. For small pipelines, combine text processing using a scikit-learn `TfidfVectorizer` and an estimator pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import preprocess_email\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sample_text = df['text'].iloc[0]\n",
    "print('Original:', sample_text)\n",
    "print('Tokens:', preprocess_email(sample_text))\n",
    "df['text_proc'] = df['text'].apply(lambda t: ' '.join(preprocess_email(t)))\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(df['text_proc'])\n",
    "print('TF-IDF shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b843b",
   "metadata": {},
   "source": [
    "## 7. 資料切分：訓練 / 驗證 / 測試與交叉驗證\n",
    "Split data with stratification on label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "if 'label' in df.columns:\n",
    "    y = df['label'].map({'ham':0, 'spam':1})\n",
    "else:\n",
    "    raise RuntimeError('Label column not found')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_proc'], y, test_size=0.2, stratify=y, random_state=42)\n",
    "print('Train/Test sizes:', len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641fe42",
   "metadata": {},
   "source": [
    "## 8. 建立基準模型（Baseline）\n",
    "Train a simple baseline: predict the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ddb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "baseline = DummyClassifier(strategy='most_frequent')\n",
    "baseline.fit(X_train.values.reshape(-1,1), y_train)\n",
    "y_pred_base = baseline.predict(X_test.values.reshape(-1,1))\n",
    "print('Baseline accuracy:', accuracy_score(y_test, y_pred_base))\n",
    "print('Baseline precision:', precision_score(y_test, y_pred_base, zero_division=0))\n",
    "print('Baseline recall:', recall_score(y_test, y_pred_base, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae4f3c",
   "metadata": {},
   "source": [
    "## 9. 模型訓練：Scikit-Learn 範例\n",
    "Use a pipeline: TfidfVectorizer + RandomForestClassifier (quick example). For larger datasets consider GridSearchCV and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "pipeline = make_pipeline(TfidfVectorizer(max_features=2000), RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88ed6b",
   "metadata": {},
   "source": [
    "## 10. 超參數調整與網格/隨機搜尋（範例）\n",
    "Example using RandomizedSearchCV for a small parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efda966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "param_dist = {\n",
    "    'randomforestclassifier__n_estimators': randint(50, 200),\n",
    "    'randomforestclassifier__max_depth': randint(3, 20)\n",
    "}\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=4, cv=3, random_state=42)\n",
    "search.fit(X_train, y_train)\n",
    "print('Best params:', search.best_params_)\n",
    "print(classification_report(y_test, search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e900ef",
   "metadata": {},
   "source": [
    "## 11. 模型評估：指標、混淆矩陣、ROC / PR 曲線\n",
    "Plot ROC curve and compute AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e786ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    y_score = search.predict_proba(X_test)[:,1]\n",
    "except Exception:\n",
    "    y_score = pipeline.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393cba1a",
   "metadata": {},
   "source": [
    "## 12. 誤差分析與模型解釋（特徵重要性）\n",
    "Show feature importances (top N) for the RandomForest part of pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = None\n",
    "clf = None\n",
    "try:\n",
    "    vec = pipeline.named_steps['tfidfvectorizer']\n",
    "    clf = pipeline.named_steps['randomforestclassifier']\n",
    "except Exception:\n",
    "    vec = pipeline.steps[0][1]\n",
    "    clf = pipeline.steps[-1][1]\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    importances = clf.feature_importances_\n",
    "    idx = importances.argsort()[::-1][:20]\n",
    "    top_feats = [(feature_names[i], importances[i]) for i in idx]\n",
    "    print('Top features:')\n",
    "    for name, imp in top_feats:\n",
    "        print(name, f'{imp:.4f}')\n",
    "else:\n",
    "    print('Classifier does not provide feature importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c63723",
   "metadata": {},
   "source": [
    "## 13. 模型儲存與匯出（joblib）\n",
    "Save the best model and vectorizer for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9716b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "joblib.dump(search.best_estimator_ if hasattr(search, 'best_estimator_') else pipeline, MODEL_DIR / 'spam_pipeline.joblib')\n",
    "print('Saved pipeline to models/spam_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798132b3",
   "metadata": {},
   "source": [
    "## 14. 簡易部署示範（Streamlit 應用骨架）\n",
    "Example Streamlit snippet to load model and run a single prediction (paste into `src/app.py` or separate `streamlit_app.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_snippet = '''import streamlit as st\n",
    "import joblib\n",
    "\n",
    "pipeline = joblib.load('models/spam_pipeline.joblib')\n",
    "\n",
    "text = st.text_area('Email text')\n",
    "if st.button('Predict'):\n",
    "    pred = pipeline.predict([text])\n",
    "    st.write('Spam' if pred[0]==1 else 'Ham')\n",
    "'''\n",
    "print(streamlit_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1151b",
   "metadata": {},
   "source": [
    "## 15. 單元測試與可重現性（pytest、設定 seed）\n",
    "Set seeds for reproducibility and show an example pytest test (tests/test_preprocessing.py already present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "print('Seeds set for numpy and random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8add79",
   "metadata": {},
   "source": [
    "## 16. 執行紀錄與實驗追蹤（logging、結果表格化）\n",
    "Demonstrate saving experiment metrics to a CSV for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62665a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "results = {\n",
    "    'model': ['random_forest'],\n",
    "    'accuracy': [0.0],\n",
    "    'precision': [0.0],\n",
    "    'recall': [0.0],\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "Path('experiments').mkdir(exist_ok=True)\n",
    "results_df.to_csv('experiments/results_summary.csv', index=False)\n",
    "print('Saved experiments/results_summary.csv (template)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
